{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vartika-bhatt/demo1/blob/main/stable_diffusion_web_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2ibl97e2yM4",
        "outputId": "c34fd9cc-9419-463d-8884-05b28581230c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Collecting git+https://github.com/huggingface/diffusers\n",
            "  Cloning https://github.com/huggingface/diffusers to /tmp/pip-req-build-t_3a59_f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers /tmp/pip-req-build-t_3a59_f\n",
            "  Resolved https://github.com/huggingface/diffusers to commit cd7071e7506275a1c7c52df9e1a696feafa3b118\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio[ffmpeg] in /usr/local/lib/python3.10/dist-packages (2.25.1)\n",
            "Collecting imageio[ffmpeg]\n",
            "  Downloading imageio-2.31.1-py3-none-any.whl (313 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting decord\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers==0.0.20\n",
            "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.20) (1.23.5)\n",
            "Collecting pyre-extensions==0.0.29 (from xformers==0.0.20)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.20) (2.0.1+cu118)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers==0.0.20)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers==0.0.20) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers==0.0.20) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers==0.0.20) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers==0.0.20) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers==0.0.20) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers==0.0.20) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers==0.0.20) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers==0.0.20) (16.0.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers==0.20.0.dev0) (4.6.4)\n",
            "Collecting huggingface-hub>=0.13.2 (from diffusers==0.20.0.dev0)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.20.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.20.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.20.0.dev0) (9.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (0.4.8)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.20.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.20.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.20.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.20.0.dev0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.20.0.dev0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->xformers==0.0.20) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->xformers==0.0.20) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers==0.0.20)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: diffusers, antlr4-python3-runtime\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.20.0.dev0-py3-none-any.whl size=1325639 sha256=82a577743edbd8c89af9354f7a863ea839d54e5a95db117acebff984c876c2ef\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yj3ggr2u/wheels/f7/7d/99/d361489e5762e3464b3811bc629e94cf5bf5ef44dd5c3c4d52\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144552 sha256=6d3f13de467d74c3e2835009a4e7347913d24a1b4d7b5ad0ce35190522b84f29\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built diffusers antlr4-python3-runtime\n",
            "Installing collected packages: tokenizers, safetensors, antlr4-python3-runtime, omegaconf, mypy-extensions, imageio, einops, decord, typing-inspect, huggingface-hub, transformers, pyre-extensions, diffusers, xformers, accelerate\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.25.1\n",
            "    Uninstalling imageio-2.25.1:\n",
            "      Successfully uninstalled imageio-2.25.1\n",
            "Successfully installed accelerate-0.21.0 antlr4-python3-runtime-4.9.3 decord-0.6.0 diffusers-0.20.0.dev0 einops-0.6.1 huggingface-hub-0.16.4 imageio-2.31.1 mypy-extensions-1.0.0 omegaconf-2.3.0 pyre-extensions-0.0.29 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0 typing-inspect-0.9.0 xformers-0.0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Text-To-Video-Finetuning'...\n",
            "remote: Enumerating objects: 973, done.\u001b[K\n",
            "remote: Counting objects: 100% (322/322), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 973 (delta 250), reused 231 (delta 221), pack-reused 651\u001b[K\n",
            "Receiving objects: 100% (973/973), 1.77 MiB | 3.99 MiB/s, done.\n",
            "Resolving deltas: 100% (571/571), done.\n",
            "Cloning into 'zeroscope_v2_dark_30x448x256'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 34 (delta 0), reused 0 (delta 0), pack-reused 32\u001b[K\n",
            "Unpacking objects: 100% (34/34), 516.81 KiB | 1.36 MiB/s, done.\n",
            "Filtering content: 100% (5/5), 7.88 GiB | 94.36 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!pip install git+https://github.com/huggingface/diffusers transformers accelerate imageio[ffmpeg] -U einops omegaconf decord xformers==0.0.20 safetensors\n",
        "!git clone -b dev https://github.com/camenduru/Text-To-Video-Finetuning\n",
        "!git clone https://huggingface.co/cerspense/zeroscope_v2_dark_30x448x256"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "# Downloading files from the demo repo\n",
        "import os\n",
        "os.mkdir('video')\n"
      ],
      "metadata": {
        "id": "ritV-JlUYdpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "code= '''\n",
        "import argparse\n",
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from uuid import uuid4\n",
        "from utils.lora import inject_inferable_lora\n",
        "import torch\n",
        "from diffusers import DPMSolverMultistepScheduler, TextToVideoSDPipeline\n",
        "from models.unet_3d_condition import UNet3DConditionModel\n",
        "from einops import rearrange\n",
        "from torch.nn.functional import interpolate\n",
        "\n",
        "from train import export_to_video, handle_memory_attention, load_primary_models\n",
        "from utils.lama import inpaint_watermark\n",
        "\n",
        "\n",
        "def initialize_pipeline(model, device=\"cuda\", xformers=False, sdp=False):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "\n",
        "        scheduler, tokenizer, text_encoder, vae, _unet = load_primary_models(model)\n",
        "        del _unet #This is a no op\n",
        "        unet = UNet3DConditionModel.from_pretrained(model, subfolder='unet')\n",
        "        # unet.disable_gradient_checkpointing()\n",
        "\n",
        "    pipeline = TextToVideoSDPipeline.from_pretrained(\n",
        "        pretrained_model_name_or_path=model,\n",
        "        scheduler=scheduler,\n",
        "        tokenizer=tokenizer,\n",
        "        text_encoder=text_encoder.to(device=device, dtype=torch.half),\n",
        "        vae=vae.to(device=device, dtype=torch.half),\n",
        "        unet=unet.to(device=device, dtype=torch.half),\n",
        "    )\n",
        "    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "    unet._set_gradient_checkpointing(value=False)\n",
        "    handle_memory_attention(xformers, sdp, unet)\n",
        "    vae.enable_slicing()\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "def vid2vid(\n",
        "    pipeline, init_video, init_weight, prompt, negative_prompt, height, width, num_inference_steps, guidance_scale\n",
        "):\n",
        "    num_frames = init_video.shape[2]\n",
        "    init_video = rearrange(init_video, \"b c f h w -> (b f) c h w\")\n",
        "    latents = pipeline.vae.encode(init_video).latent_dist.sample()\n",
        "    latents = rearrange(latents, \"(b f) c h w -> b c f h w\", f=num_frames)\n",
        "    latents = pipeline.scheduler.add_noise(\n",
        "        original_samples=latents * 0.18215,\n",
        "        noise=torch.randn_like(latents),\n",
        "        timesteps=(torch.ones(latents.shape[0]) * pipeline.scheduler.num_train_timesteps * (1 - init_weight)).long(),\n",
        "    )\n",
        "    if latents.shape[0] != len(prompt):\n",
        "        latents = latents.repeat(len(prompt), 1, 1, 1, 1)\n",
        "\n",
        "    do_classifier_free_guidance = guidance_scale > 1.0\n",
        "\n",
        "    prompt_embeds = pipeline._encode_prompt(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        device=latents.device,\n",
        "        num_images_per_prompt=1,\n",
        "        do_classifier_free_guidance=do_classifier_free_guidance,\n",
        "    )\n",
        "\n",
        "    pipeline.scheduler.set_timesteps(num_inference_steps, device=latents.device)\n",
        "    timesteps = pipeline.scheduler.timesteps\n",
        "    timesteps = timesteps[round(init_weight * len(timesteps)) :]\n",
        "\n",
        "    with pipeline.progress_bar(total=len(timesteps)) as progress_bar:\n",
        "        for t in timesteps:\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "            latent_model_input = pipeline.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "            # predict the noise residual\n",
        "            noise_pred = pipeline.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds).sample\n",
        "\n",
        "            # perform guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # reshape latents\n",
        "            bsz, channel, frames, width, height = latents.shape\n",
        "            latents = latents.permute(0, 2, 1, 3, 4).reshape(bsz * frames, channel, width, height)\n",
        "            noise_pred = noise_pred.permute(0, 2, 1, 3, 4).reshape(bsz * frames, channel, width, height)\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            latents = pipeline.scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "            # reshape latents back\n",
        "            latents = latents[None, :].reshape(bsz, frames, channel, width, height).permute(0, 2, 1, 3, 4)\n",
        "\n",
        "            progress_bar.update()\n",
        "\n",
        "    video_tensor = pipeline.decode_latents(latents)\n",
        "\n",
        "    return video_tensor\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def inference(\n",
        "    model,\n",
        "    prompt,\n",
        "    negative_prompt=None,\n",
        "    batch_size=1,\n",
        "    num_frames=16,\n",
        "    width=256,\n",
        "    height=256,\n",
        "    num_steps=50,\n",
        "    guidance_scale=9,\n",
        "    init_video=None,\n",
        "    init_weight=0.5,\n",
        "    device=\"cuda\",\n",
        "    xformers=False,\n",
        "    sdp=False,\n",
        "    lora_path='',\n",
        "    lora_rank=64\n",
        "):\n",
        "    with torch.autocast(device, dtype=torch.half):\n",
        "        pipeline = initialize_pipeline(model, device, xformers, sdp)\n",
        "        inject_inferable_lora(pipeline, lora_path, r=lora_rank)\n",
        "        prompt = [prompt] * batch_size\n",
        "        negative_prompt = ([negative_prompt] * batch_size) if negative_prompt is not None else None\n",
        "\n",
        "        if init_video is not None:\n",
        "            videos = vid2vid(\n",
        "                pipeline=pipeline,\n",
        "                init_video=init_video.to(device=device, dtype=torch.half),\n",
        "                init_weight=init_weight,\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                height=height,\n",
        "                width=width,\n",
        "                num_inference_steps=num_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            videos = pipeline(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_frames=num_frames,\n",
        "                height=height,\n",
        "                width=width,\n",
        "                num_inference_steps=num_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                output_type=\"pt\",\n",
        "            ).frames\n",
        "\n",
        "        return videos\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import decord\n",
        "\n",
        "    decord.bridge.set_bridge(\"torch\")\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-m\", \"--model\", type=str, required=True)\n",
        "    parser.add_argument(\"-p\", \"--prompt\", type=str, required=True)\n",
        "    parser.add_argument(\"-n\", \"--negative-prompt\", type=str, default=None)\n",
        "    parser.add_argument(\"-o\", \"--output-dir\", type=str, default=\"./output\")\n",
        "    parser.add_argument(\"-B\", \"--batch-size\", type=int, default=1)\n",
        "    parser.add_argument(\"-T\", \"--num-frames\", type=int, default=16)\n",
        "    parser.add_argument(\"-W\", \"--width\", type=int, default=256)\n",
        "    parser.add_argument(\"-H\", \"--height\", type=int, default=256)\n",
        "    parser.add_argument(\"-s\", \"--num-steps\", type=int, default=50)\n",
        "    parser.add_argument(\"-g\", \"--guidance-scale\", type=float, default=9)\n",
        "    parser.add_argument(\"-i\", \"--init-video\", type=str, default=None)\n",
        "    parser.add_argument(\"-iw\", \"--init-weight\", type=float, default=0.5)\n",
        "    parser.add_argument(\"-f\", \"--fps\", type=int, default=8)\n",
        "    parser.add_argument(\"-d\", \"--device\", type=str, default=\"cuda\")\n",
        "    parser.add_argument(\"-x\", \"--xformers\", action=\"store_true\")\n",
        "    parser.add_argument(\"-S\", \"--sdp\", action=\"store_true\")\n",
        "    parser.add_argument(\"-lP\", \"--lora_path\", type=str, default=\"\")\n",
        "    parser.add_argument(\"-lR\", \"--lora_rank\", type=int, default=64)\n",
        "    parser.add_argument(\"-rw\", \"--remove-watermark\", action=\"store_true\")\n",
        "    args = vars(parser.parse_args())\n",
        "\n",
        "    output_dir = args.pop(\"output_dir\")\n",
        "    prompt = args.get(\"prompt\")\n",
        "    fps = args.pop(\"fps\")\n",
        "    remove_watermark = args.pop(\"remove_watermark\")\n",
        "    init_video = args.pop(\"init_video\")\n",
        "\n",
        "    if init_video is not None:\n",
        "        vr = decord.VideoReader(init_video)\n",
        "        init = rearrange(vr[:], \"f h w c -> c f h w\").div(127.5).sub(1).unsqueeze(0)\n",
        "        init = interpolate(init, size=(args[\"num_frames\"], args[\"height\"], args[\"width\"]), mode=\"trilinear\")\n",
        "        args[\"init_video\"] = init\n",
        "\n",
        "    videos = inference(**args)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    out_stem = f\"{output_dir}/\"\n",
        "    if init_video is not None:\n",
        "        out_stem += f\"({Path(init_video).stem}) * {args['init_weight']} | \"\n",
        "        path=out_stem\n",
        "    out_stem += f\"{prompt}\"\n",
        "\n",
        "    for video in videos:\n",
        "\n",
        "        if remove_watermark:\n",
        "            video = rearrange(video, \"c f h w -> f c h w\").add(1).div(2)\n",
        "            video = inpaint_watermark(video)\n",
        "            video = rearrange(video, \"f c h w -> f h w c\").clamp(0, 1).mul(255)\n",
        "\n",
        "        else:\n",
        "            video = rearrange(video, \"c f h w -> f h w c\").clamp(-1, 1).add(1).mul(127.5)\n",
        "\n",
        "        video = video.byte().cpu().numpy()\n",
        "\n",
        "        export_to_video(video, f\"{out_stem}.mp4\", fps)\n",
        "  '''\n",
        "\n",
        "with open('/content/Text-To-Video-Finetuning/inference.py', 'w') as f:\n",
        "    f.write(code)\n",
        "\n"
      ],
      "metadata": {
        "id": "k91PkAwUsUH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqo7g4p2_V78",
        "outputId": "5b68e14f-5a60-4a1c-ddd6-58bf8544450e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Text-To-Video-Finetuning\n",
            "2023-08-11 02:19:31.667073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading pipeline components...: 100% 5/5 [00:00<00:00, 9267.13it/s]\n",
            "100% 30/30 [00:17<00:00,  1.71it/s]\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Text-To-Video-Finetuning\n",
        "# while True:\n",
        "prompt = \"cars driving on a road\" #@param {type:\"string\"}\n",
        "negative = \"\" #@param {type:\"string\"}\n",
        "prompt = f\"\\\"{prompt}\\\"\"\n",
        "negative = f\"\\\"{negative}\\\"\"\n",
        "num_steps = 30 #@param {type:\"raw\"}\n",
        "guidance_scale = 23 #@param {type:\"raw\"}\n",
        "fps = 24 #@param {type:\"raw\"}\n",
        "num_frames = 20 #@param {type:\"raw\"}\n",
        "!python inference.py -m \"/content/zeroscope_v2_dark_30x448x256\" -p {prompt} -n {negative} -W 448 -H 256 -o /content/outputs -d cuda -x -s {num_steps} -g {guidance_scale} -f {fps} -T {num_frames}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=prompt[1:(len(prompt)-1)]"
      ],
      "metadata": {
        "id": "ykkEXPQgRVI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path=\"/content/outputs/\"+prompt+\".mp4\""
      ],
      "metadata": {
        "id": "K9VTZJ2Pt8QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(path)"
      ],
      "metadata": {
        "id": "-VMBRIT1wZ--",
        "outputId": "1524d301-9a3e-42eb-e5cb-4d2a8ee1a00a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'/content/outputs/cars driving on a road.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "\n",
        "def video_identity(video):\n",
        "    return video\n",
        "\n",
        "\n",
        "demo = gr.Interface(video_identity,\n",
        "                    gr.Video(),\n",
        "                    \"playable_video\",\n",
        "                    examples=[\n",
        "                        path],\n",
        "                    cache_examples=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #print(public_url)\n",
        "    demo.launch(share=True,debug=True)\n"
      ],
      "metadata": {
        "id": "BK0dQMfe3diF",
        "outputId": "f51d8642-3889-4958-9379-ce197ee252c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/video.py:332: UserWarning: Video does not have browser-compatible container or codec. Converting to mp4\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching examples at: '/content/Text-To-Video-Finetuning/gradio_cached_examples/91'\n",
            "Caching example 1/1\n",
            "Caching complete\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://a604f1449152095520.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a604f1449152095520.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a604f1449152095520.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dCdPml4hNHYF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}